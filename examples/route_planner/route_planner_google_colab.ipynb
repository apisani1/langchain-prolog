{"cells":[{"cell_type":"markdown","id":"02e46451","metadata":{"id":"02e46451"},"source":["# Route Planner: A Prolog Application with LangChain Integration\n","\n","## Overview\n","The `routes.pl` application is a travel route planning system implemented in Prolog that demonstrates logical reasoning capabilities integrated with LangChain. It serves as an example of how Prolog's declarative programming paradigm can enhance LLM applications with powerful reasoning.\n","\n","## Key Features\n","- **Transport Network Modeling:** Intuitively defines connections between cities with various transport types (train, plane, ferry).  \n","- **Route Finding:** Discovers all possible routes between origin and destination cities.  \n","- **Optimization:** Calculates fastest and cheapest routes based on time and cost parameters.  \n","- **Constraint Handling:** Applies constraints like maximum time, maximum cost, and number of connections.  \n","- **Transport Preferences:** Filters routes based on preferred transport types.  \n","- **Comprehensive Query Interface:** Provides predicates for different query types and formats.  \n","\n","## Technical Highlights\n","\n","- Uses Prolog's backtracking to explore all possible routes.  \n","- Implements cycle detection to prevent infinite loops.  \n","- Provides helper predicates for visualizing and analyzing routes.  \n","- Structures data for easy integration with Python.  \n","- Returns results in formats compatible with LangChain Runnable and Tool interfaces.  \n","\n","## Integration with LangChain\n","When integrated with LangChain or LangGraph, this application enables:\n","\n","- Natural language queries about travel routes.  \n","- Reasoning about optimal travel plans based on constraints.  \n","- Explanations of why certain routes are preferred.  \n","- Step-by-step travel planning with LLM guidance.  "]},{"cell_type":"markdown","source":["## How to use this notebook:\n","1. Run the setup cell to install SWI-Prolog and required packages\n","2. Run each cell in sequence to see the route planning in action\n","3. Try modifying the queries to explore different routes"],"metadata":{"id":"0fe1hKqOhpTp"},"id":"0fe1hKqOhpTp"},{"cell_type":"markdown","id":"60be2d1c","metadata":{"id":"60be2d1c"},"source":["## Setup\n","\n","**Prerequisites**\n","\n","- Python 3.10 or later\n","- SWI-Prolog installed on your system\n","- The following python libraries installed:\n","    - langchain 0.3.0 or later\n","    - janus-swi 1.5.0 or later\n","    - pydantic 2.0.0 or later\n","\n","The Prolog interfase with LangChain can be installed using pip:\n","\n","**&ensp;&ensp;&ensp;&ensp;&ensp;pip install langchain-prolog**\n","\n","<br>\n","\n","---\n","\n","\n","**Note:** This notebook installs SWI-Prolog and the required Python packages, and if you choose to use Ollama as the provider for the chat model, it then downloads and uses LLama running directly in Google Colab instead of requiring an API key.\n","This setup process takes a few minutes. For better performance, consider running this notebook locally and using an API based chat model.\n","\n","If you choose to run it in Google Colab with an API-based chat model, you must provide your API key, preferably as an environment variable.\n","\n","---"]},{"cell_type":"code","source":["#@title Run this cell to setup SWI-Prolog and the required Python packages. Click on > to view/hide the script. {display-mode: \"form\"}\n","\n","from IPython.display import display, HTML, clear_output\n","import os\n","import subprocess\n","import time\n","\n","# Function to create progress bar\n","def show_progress(step, total, progress, status=\"Working...\"):\n","  bar_style = f\"\"\"\n","  <div style=\"\n","      width: 100%;\n","      height: 20px;\n","      background-color: #f1f1f1;\n","      border-radius: 5px;\n","      margin: 10px 0;\n","  \">\n","      <div style=\"\n","          width: {progress}%;\n","          height: 100%;\n","          background-color: #4CAF50;\n","          border-radius: 5px;\n","          text-align: center;\n","          line-height: 20px;\n","          color: white;\n","      \">\n","          {int(progress)}%\n","      </div>\n","  </div>\n","  <div style=\"text-align: center; margin-bottom: 15px; font-weight: bold;\">\n","      {status} ({step}/{total})\n","  </div>\n","  \"\"\"\n","  clear_output(wait=True)\n","  display(HTML(bar_style))\n","\n","total_steps = 3\n","\n","# SWI-Prolog installation\n","show_progress(1, total_steps, 0,  \"Installing SWI-Prolog\")\n","!apt-add-repository ppa:swi-prolog/stable -y\n","!apt-get update\n","!apt-get install -y swi-prolog\n","\n","# SWI-Prolog environment setup\n","swipl_home = subprocess.check_output(\n","    \"swipl -g \\\"current_prolog_flag(home, Home), writeln(Home)\\\" -t halt\",\n","    shell=True\n",").decode().strip()\n","arch = subprocess.check_output(\"uname -m\", shell=True).decode().strip()\n","os.environ[\"SWI_HOME_DIR\"] = swipl_home\n","ld_library_path = f\"{swipl_home}/lib/{arch}-linux\"\n","if \"LD_LIBRARY_PATH\" in os.environ:\n","    os.environ[\"LD_LIBRARY_PATH\"] = f\"{ld_library_path}:{os.environ['LD_LIBRARY_PATH']}\"\n","else:\n","    os.environ[\"LD_LIBRARY_PATH\"] = ld_library_path\n","\n","show_progress(2, total_steps, 60, \"Installing langchain-prolog\")\n","!pip install --quiet \"langchain>=0.3.0\"\n","!pip install --quiet \"pydantic>=2.0.0\"\n","!pip install --quiet \"janus-swi>=1.5.0\"\n","!pip install --quie langchain-prolog\n","\n","show_progress(3, total_steps, 80, \"Installing LangGraph\")\n","!pip install --quiet \"langgraph==0.2.74\"\n","!pip install --quiet \"langgraph-checkpoint==2.0.16\"\n","!pip install --quiet \"langgraph-sdk==0.1.53\"\n","!pip install --quiet langchain-community\n","\n","show_progress(3, total_steps, 100, \"Installation completed\")\n","time.sleep(2)\n","\n","openai_installed = False\n","anthropic_installed = False\n","ollama_installed = False\n","\n","try:\n","    import langchain_prolog\n","    from langchain_prolog import PrologConfig, PrologRunnable, PrologTool\n","    display(HTML(f\"<div style='padding: 10px; background-color: #d4edda; border-radius: 5px; text-align: center;'><b>Success!</b> langchain-prolog {langchain_prolog.__version__} installed</div>\"))\n","except ImportError as e:\n","    display(HTML(f\"<div style='padding: 10px; background-color: #f8d7da; border-radius: 5px; text-align: center;'><b>Error:</b> {str(e)}</div>\"))\n","\n","def show_ai_message(response, height=200):\n","  \"\"\"\n","  Display AI message in a scrollable container with better formatting.\n","\n","  Args:\n","      response (str): The AI's text response\n","      height (int): Height of the scrollable area in pixels\n","  \"\"\"\n","  # Escape HTML characters to prevent injection\n","  import html\n","  escaped_text = html.escape(response)\n","\n","  # Create the HTML with the requested title format\n","  html_content = f\"\"\"\n","  <div style=\"margin: 20px 0;\">\n","      <div style=\"\n","          font-weight: bold;\n","          margin-bottom: 10px;\n","          text-align: center;\n","          font-family: monospace;\n","          font-size: 16px;\n","          color: #2c3e50;\n","      \">================================== AI Message ==================================</div>\n","      <div style=\"\n","          height: {height}px;\n","          overflow-y: auto;\n","          border: 1px solid #ccc;\n","          border-radius: 4px;\n","          padding: 15px;\n","          background-color: #f9f9f9;\n","          font-family: monospace;\n","          white-space: pre-wrap;\n","          line-height: 1.5;\n","      \">\n","      {escaped_text}\n","      </div>\n","  </div>\n","  \"\"\"\n","\n","  display(HTML(html_content))"],"metadata":{"id":"qiphuxDyiOXG","collapsed":true},"id":"qiphuxDyiOXG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Run this cell to choose the Chat model provider. Click on > to view/hide the script. {display-mode: \"form\"}\n","import getpass\n","import os\n","from ipywidgets import widgets\n","from IPython.display import display\n","\n","provider_widget = widgets.Dropdown(options=['Ollama', 'OpenAI', 'Anthropic'], description='Provider: ')\n","display(provider_widget)\n","provider = None\n","\n","def on_button_clicked(b):\n","    global provider\n","\n","    provider = provider_widget.value\n","    print(f\"\\n{provider} selected\")\n","\n","button = widgets.Button(description=\"Chose Provider\")\n","button.on_click(on_button_clicked)\n","display(button)"],"metadata":{"id":"4vMznfXoWbTm"},"id":"4vMznfXoWbTm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Run this cell to setup the Chat model provider. Click on > to view/hide the script. {display-mode: \"form\"}\n","# These packages are not required by lanchain-prolog but are used by this notebook\n","\n","import time\n","from IPython.display import display, HTML\n","\n","if provider == \"OpenAI\" and not openai_installed:\n","\n","  show_progress(1, 1, 0,  \"Installing langchain-openai\")\n","  if not os.environ.get(\"OPENAI_API_KEY\"):\n","    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n","\n","  !pip install --quiet langchain-openai\n","  openai_installed = True\n","  show_progress(1, 1, 100,  \"OpenAI installed\")\n","\n","elif provider == \"Anthropic\" and not anthropic_installed:\n","\n","  show_progress(1, 1, 0,  \"Installing langchain-anthropic\")\n","  if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n","    os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n","\n","  !pip install --quiet langchain-anthropic\n","  anthropic_installed = True\n","  show_progress(1, 1, 100,  \"Anthropic installed\")\n","\n","elif provider == \"Ollama\" and not ollama_installed:\n","\n","  total_steps = 3\n","  show_progress(1, total_steps, 0,  \"Installing langchain-ollama\")\n","  !pip install --quiet langchain-ollama\n","  !pip install --quiet ollama\n","\n","  show_progress(2, total_steps, 20,  \"Installing Ollama\")\n","  !curl -fsSL https://ollama.com/install.sh | sh\n","\n","  # Create directory for Ollama\n","  !mkdir -p /root/.ollama\n","\n","  # Start Ollama service in the background\n","  !nohup ollama serve > ollama.log 2>&1 &\n","\n","  # Wait for Ollama to start\n","  time.sleep(10)\n","\n","  # Check if Ollama is running\n","  !curl -s http://localhost:11434/api/tags\n","\n","  show_progress(3, total_steps, 60,  \"Downloading LLama\")\n","  # Pull llama3.1-8b which supports function calling and fits in Colab's memory\n","  print(\"\\nDownloading llama3.1-8b (this will take 5-10 minutes)...\")\n","  !ollama pull llama3.1:8b\n","\n","  # Verify the model is downloaded\n","  !ollama list\n","\n","  ollama_installed = True\n","  show_progress(3, total_steps, 100,  \"Ollama istalled\")\n","\n","time.sleep(2)\n","display(HTML(f\"<div style='padding: 10px; background-color: #d4edda; border-radius: 5px; text-align: center;'><b>Success!</b> {provider} installed</div>\"))"],"metadata":{"id":"xmCmfu4PTIh6"},"id":"xmCmfu4PTIh6","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The Prolog application"],"metadata":{"id":"joJDkeA_S0VK"},"id":"joJDkeA_S0VK"},{"cell_type":"code","source":["#@title Click on > to view/hide the Prolog app. Run this cell to save it as a file.{display-mode: \"form\"}\n","\n","%%writefile routes.pl\n",":- module(routes, [\n","    query_route/5,          % Main interface\n","    route_stats/2           % Route statistics\n","]).\n","\n","% Transport connections with time (hours), cost (euros), and transport type\n","connection(london, paris, transport(train, 2.5, 150)).\n","connection(london, paris, transport(plane, 1.5, 200)).\n","connection(paris, rome, transport(plane, 2, 180)).\n","connection(paris, barcelona, transport(plane, 1.5, 120)).\n","connection(paris, madrid, transport(train, 2, 150)).\n","connection(barcelona, madrid, transport(train, 3, 80)).\n","connection(madrid, lisbon, transport(train, 4, 90)).\n","connection(rome, athens, transport(ferry, 8, 120)).\n","connection(rome, athens, transport(plane, 1.5, 160)).\n","connection(barcelona, rome, transport(plane, 2, 140)).\n","\n","% Bidirectional connections. This is optional.\n","valid_connection(A, B, Transport) :-\n","    (   connection(A, B, Transport)\n","    ;   connection(B, A, Transport)\n","    ).\n","\n","% Query type definitions\n","valid_query_type(all).        % All routes\n","valid_query_type(fastest).    % Fastest route only\n","valid_query_type(cheapest).   % Cheapest route only\n","\n","% Main interface predicate\n","query_route(QueryType, From, To, Options, Results) :-\n","    must_be(atom, QueryType),\n","    must_be(atom, From),\n","    must_be(atom, To),\n","    must_be(list, Options),\n","\n","    % Validate query type\n","    (   valid_query_type(QueryType)\n","    ->  true\n","    ;   throw(error(invalid_query_type(QueryType), _))\n","    ),\n","\n","    % Execute query based on type\n","    execute_query(QueryType, From, To, Options, Results).\n","\n","% Execute different types of queries\n","execute_query(QueryType, From, To, Options, Results) :-\n","    findall(\n","        Dict,\n","        (   route(From, To, Route, details(Time, Cost, Transport)),\n","            apply_options(Options, Time, Cost, Transport),\n","            maplist(transport_to_dict, Transport, TransportList),\n","            Dict = _{\n","                'route': Route,\n","                'time': Time,\n","                'cost': Cost,\n","                'transport': TransportList\n","            }\n","        ),\n","        AllRoutes\n","    ),\n","    % Select routes based on query type\n","    select_routes(QueryType, AllRoutes, Results).\n","\n","% Select routes based on query type\n","select_routes(all, [Route|Rest], [Route|Rest]).\n","select_routes(fastest, Routes, [FastestRoute]) :-\n","    find_fastest_route(Routes, FastestRoute).\n","select_routes(cheapest, Routes, [CheapestRoute]) :-\n","    find_cheapest_route(Routes, CheapestRoute).\n","\n","% Find fastest route\n","find_fastest_route([Route|Rest], FastestRoute) :-\n","    Route.get(time) = Time,\n","    find_fastest_route(Rest, Time, Route, FastestRoute).\n","\n","find_fastest_route([], _, FastestSoFar, FastestSoFar).\n","find_fastest_route([Route|Rest], MinTime, CurrentBest, FastestRoute) :-\n","    Route.get(time) = Time,\n","    (   Time < MinTime\n","    ->  find_fastest_route(Rest, Time, Route, FastestRoute)\n","    ;   find_fastest_route(Rest, MinTime, CurrentBest, FastestRoute)\n","    ).\n","\n","% Find cheapest route\n","find_cheapest_route([Route|Rest], CheapestRoute) :-\n","    Route.get(cost) = Cost,\n","    find_cheapest_route(Rest, Cost, Route, CheapestRoute).\n","\n","find_cheapest_route([], _, CheapestSoFar, CheapestSoFar).\n","find_cheapest_route([Route|Rest], MinCost, CurrentBest, CheapestRoute) :-\n","    Route.get(cost) = Cost,\n","    (   Cost < MinCost\n","    ->  find_cheapest_route(Rest, Cost, Route, CheapestRoute)\n","    ;   find_cheapest_route(Rest, MinCost, CurrentBest, CheapestRoute)\n","    ).\n","\n","% Route finding with constraints\n","route(From, To, Route, Details) :-\n","    find_route(From, To, [From], Route, [], Details).\n","\n","% Base case: direct connection\n","find_route(From, To, _, [From,To], TransportList,\n","          details(Time, Cost, FinalTransport)) :-\n","    valid_connection(From, To, transport(Type, Time, Cost)),\n","    append(TransportList, [transport(Type, Time, Cost)], FinalTransport).\n","\n","% Recursive case with cycle detection\n","find_route(From, To, Visited, [From|Route], AccTransport,\n","          details(TotalTime, TotalCost, FinalTransport)) :-\n","    valid_connection(From, Next, transport(Type, Time, Cost)),\n","    \\+ member(Next, Visited),  % Prevent cycles\n","    \\+ member(To, Visited),  % Prevent redudant paths\n","    length(Visited, Len),\n","    Len < 5,  % Limit path length to prevent excessive searching\n","    append(AccTransport, [transport(Type, Time, Cost)], NewTransport),\n","    find_route(Next, To, [Next|Visited], Route, NewTransport,\n","              details(RestTime, RestCost, FinalTransport)),\n","    TotalTime is Time + RestTime,\n","    TotalCost is Cost + RestCost.\n","\n","% Apply filtering options\n","apply_options([], _, _, _) :- !.\n","apply_options(Options, Time, Cost, Transport) :-\n","    % Time constraint\n","    \\+ (\n","        member(max_time(MaxTime), Options),\n","        Time > MaxTime\n","    ),\n","    % Cost constraint\n","    \\+ (\n","        member(max_cost(MaxCost), Options),\n","        Cost > MaxCost\n","    ),\n","    % Changes constraint\n","    \\+ (\n","        member(max_changes(MaxChanges), Options),\n","        length(Transport, Changes),\n","        Changes > MaxChanges + 1\n","    ),\n","    % Transport type constraint\n","    \\+ (\n","        member(transport_type(PreferredTypeList), Options),\n","        has_nonpreferred_transport_type(Transport, PreferredTypeList)\n","    ).\n","\n","% Helper predicate to check if route has required transport type\n","has_nonpreferred_transport_type(Transport, PreferredTypeList) :-\n","    member(transport(TransportType, _, _), Transport),\n","    \\+ member(TransportType, PreferredTypeList).\n","\n","% Convert transport term to dict\n","transport_to_dict(transport(Type, Time, Cost), Dict) :-\n","    Dict = _{\n","        'type': Type,\n","        'time': Time,\n","        'cost': Cost\n","    }.\n","\n","% Route statistics\n","route_stats(From, To) :-\n","    findall(Details, route(From, To, _, Details), AllDetails),\n","    AllDetails \\= [],\n","    findall(Time, member(details(Time,_,_), AllDetails), Times),\n","    findall(Cost, member(details(_,Cost,_), AllDetails), Costs),\n","    min_list(Times, MinTime),\n","    max_list(Times, MaxTime),\n","    sum_list(Times, TotalTime),\n","    length(Times, Count),\n","    AvgTime is TotalTime / Count,\n","    min_list(Costs, MinCost),\n","    max_list(Costs, MaxCost),\n","    sum_list(Costs, TotalCost),\n","    AvgCost is TotalCost / Count,\n","    format('~nRoute Statistics:~n'),\n","    format('Number of routes: ~w~n', [Count]),\n","    format('Time range: ~2f - ~2f hours (avg: ~2f)~n',\n","           [MinTime, MaxTime, AvgTime]),\n","    format('Cost range: €~2f - €~2f (avg: €~2f)~n',\n","           [MinCost, MaxCost, AvgCost])."],"metadata":{"id":"WzthHx5_i7Vd"},"id":"WzthHx5_i7Vd","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"4595315c","metadata":{"id":"4595315c"},"source":["## Instantiation\n","The most important classes in langchain-prolog are: `PrologConfig`, `PrologRunnable` and `PrologTool`\n","- `PrologConfig` sets the configuration for the Prolog interpreter. The only mandatory field is the path to the Prolog script to be used.\n","- `PrologRunnable.create_schema` defines a Pydantic schema to be used to pass arguments to the Prolog predicates. It is optional, but recomended.\n","- `PrologTool` wraps the Prolog script with the interfase to LangChain/LangGraph. I supports all the methods of the `Tool` class and tracing capabilities of LangSmith."]},{"cell_type":"code","execution_count":null,"id":"9b9a2015","metadata":{"id":"9b9a2015"},"outputs":[],"source":["from langchain_prolog import PrologConfig, PrologRunnable, PrologTool\n","\n","schema = PrologRunnable.create_schema('query_route', ['query_type', 'from', 'to', 'options', 'results'])\n","\n","config = PrologConfig(\n","            rules_file='routes.pl',\n","            query_schema=schema,\n","            default_predicate=\"query_route\"\n","        )\n","\n","planner_tool = PrologTool(\n","    prolog_config=config,\n","    name=\"prolog_travel_planner\",\n","    description=\"\"\"\n","        Query travel routes using Prolog.\n","        Input have to be a dictionay like:\n","          {\n","            'query_type': 'all',\n","            'from': 'paris',\n","            'to': 'london',\n","            'options' [],\n","            'results': None\n","          }\n","        You always have to specify 4 parameters:\n","            - query_type: can be 'all', 'fastest' or 'cheapest'\n","            - from: the city where the travel starts. Must be all lower case.\n","            - to: the city where the travel ends. Must be all lower case.\n","            - options: set this to []\n","        The query will return:\n","            - 'False' if there are no routes availables\n","            - A dictionary with 'Results' as the key and a list of possible routes as the value\n","        Do not use quotes.\n","    \"\"\",\n",")"]},{"cell_type":"markdown","id":"6c81e971","metadata":{"id":"6c81e971"},"source":["## Invocation\n","If a schema is defined, we can pass a dictionary using the names of the parameters in the schema as the keys in the dictionary. The values can represent Prolog variables (uppercase first letter) or strings (lower case first letter). A `None` value is interpreted as a variable and replaced with the key capitalized:"]},{"cell_type":"code","execution_count":null,"id":"cbc4f2d7","metadata":{"id":"cbc4f2d7"},"outputs":[],"source":["planner_tool.invoke(\n","    {\n","        'query_type': 'all',\n","        'from': 'paris',\n","        'to': 'lisbon',\n","        'options': [],\n","        'results': None,\n","    }\n",")"]},{"cell_type":"markdown","source":["## Setting up the Chat Model"],"metadata":{"id":"lf2JVO5DfTKk"},"id":"lf2JVO5DfTKk"},{"cell_type":"code","source":["#@title Run this cell to instantiate the Chat model. Click on > to view/hide the script. {display-mode: \"form\"}\n","\n","%%capture --no-stderr --no-display\n","\n","from IPython.display import display, HTML\n","\n","if provider == \"OpenAI\":\n","\n","  from langchain_openai import ChatOpenAI\n","\n","  model = \"gpt-4o-mini\"\n","  llm = ChatOpenAI(model=model, temperature=0)\n","  max_tries = 10\n","  get_answer = lambda response: response[\"output\"]\n","\n","elif provider == \"Anthropic\":\n","\n","  from langchain_anthropic import ChatAnthropic\n","\n","  model = \"claude-3-5-sonnet-latest\"\n","  llm = ChatAnthropic(model=model, temperature=0)\n","  max_tries = 10\n","  get_answer = lambda response: response[\"output\"][0][\"text\"]\n","  model = llm.model\n","\n","elif provider == \"Ollama\":\n","\n","  from langchain_ollama import ChatOllama\n","\n","  model = \"llama3.1:8b\"\n","  llm = ChatOllama(\n","          model=model,\n","          base_url=\"http://localhost:11434\",\n","          temperature=0\n","        )\n","  max_tries = 100\n","  get_answer = lambda response: response[\"output\"]\n","  model = llm.model\n","\n","llm.invoke(\"Hi\")\n","display(HTML(f\"<div style='padding: 10px; background-color: #d4edda; border-radius: 5px; text-align: center;'><b>Success!</b> {model} instantiated!</div>\"))"],"metadata":{"id":"PpTgX6a8GQ6_"},"id":"PpTgX6a8GQ6_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Set up your own query"],"metadata":{"id":"l_F01iW9Y0nZ"},"id":"l_F01iW9Y0nZ"},{"cell_type":"code","source":["#@title Run this cell to set your travel query. Click on > to view/hide the script. {display-mode: \"form\"}\n","\n","from ipywidgets import widgets\n","from IPython.display import display\n","\n","cities = ['London', 'Paris', 'Rome', 'Athens', 'Barcelona', 'Madrid', 'Lisbon']\n","from_city = widgets.Dropdown(options=cities, description='From:')\n","to_city = widgets.Dropdown(options=cities, description='To:')\n","query_type = widgets.Dropdown(options=['all', 'fastest', 'cheapest'], description='Query:')\n","\n","display(from_city, to_city, query_type)\n","\n","def on_button_clicked(b):\n","    global query\n","    #query = f\"What's the {query_type.value} route from {from_city.value} to {to_city.value}?\"\n","    query = f\"I want to tavel from from {from_city.value} to {to_city.value}. \"\n","    if query_type.value == 'fastest':\n","        query = query + \"What's the fastest route?\"\n","    elif query_type.value == 'cheapest':\n","        query = query + \"What's the cheapest route?\"\n","    else:\n","        query = query + \"What are my options?\"\n","    print(f\"\\n{query}\")\n","    query = f\"\"\"\n","            Always use the prolog_travel_planner tool for any questions\n","            regarding travelling, respond with a detailed answer using all\n","            the information provided by the prolog_travel_planner tool.\n","            Don't try to use your own knowledge or real time data.\n","\n","            Question: {query}\n","            \"\"\"\n","\n","button = widgets.Button(description=\"Define Query\")\n","button.on_click(on_button_clicked)\n","display(button)"],"metadata":{"id":"ZA2ZLl8kYRAV"},"id":"ZA2ZLl8kYRAV","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c3363f7d","metadata":{"id":"c3363f7d"},"source":["## Using an LLM and function calling\n","\n","Yor can use any LangChain chat model that supports tool calling. To use the Prolog tool, bind it to the chat model:"]},{"cell_type":"code","execution_count":null,"id":"3ce4479d","metadata":{"id":"3ce4479d"},"outputs":[],"source":["llm_with_tools = llm.bind_tools([planner_tool])"]},{"cell_type":"markdown","id":"dcdaef42","metadata":{"id":"dcdaef42"},"source":["and then query the model:"]},{"cell_type":"code","execution_count":null,"id":"86d79c1c","metadata":{"id":"86d79c1c"},"outputs":[],"source":["tries = 1\n","while tries <= max_tries:\n","  try:\n","\n","    # The user query is pass in as a list of messages\n","    messages = [(\"human\", query)]\n","    response = llm_with_tools.invoke(messages)\n","\n","    # The LLM will respond with a tool call request if needed\n","    if hasattr(response, 'tool_calls') and response.tool_calls:\n","        messages.append(response)\n","\n","        # The tool takes this request and queries the Prolog database:\n","        tool_msg = planner_tool.invoke(response.tool_calls[0])\n","        messages.append(tool_msg)\n","\n","        #The tool returns a list with all the solutions for the query\n","        response = llm_with_tools.invoke(messages)\n","\n","    #Check that we have a valid response\n","    if response.content:\n","      show_ai_message(response.content)\n","      break\n","    else:\n","      tries += 1\n","\n","  except Exception as e:\n","    # The LLM failed to communicate with the tool using the right schema. It happens!\n","    tries += 1\n","\n","if tries > max_tries:\n","  show_ai_message(\"Could not get an answer\", height=100)"]},{"cell_type":"markdown","id":"ccc02700","metadata":{"id":"ccc02700"},"source":["## Using a LangChain Agent\n","To use a Prolog tool with an agent, pass it to the agent's constructor:"]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain.agents import create_tool_calling_agent, AgentExecutor\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","      (\"system\", \"You are a helpful assistant\"),\n","      (\"human\", \"{input}\"),\n","      (\"placeholder\", \"{agent_scratchpad}\"),\n","    ]\n",")\n","tools = [planner_tool]\n","agent = create_tool_calling_agent(llm, tools, prompt)\n","agent_executor = AgentExecutor(agent=agent, tools=tools)"],"metadata":{"id":"7a4yTfZRth7n"},"id":"7a4yTfZRth7n","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"389f1e86","metadata":{"id":"389f1e86"},"source":["The agent takes the query and use the Prolog tool if needed. Then the agent receives​ the tool response and generates the answer:"]},{"cell_type":"code","source":["tries = 1\n","while tries <= max_tries:\n","  try:\n","    response = agent_executor.invoke({\"input\": query})\n","    answer = get_answer(response)\n","    show_ai_message(answer)\n","    break\n","  except Exception as e:\n","    tries += 1\n","if tries > max_tries:\n","  show_ai_message(\"Could not get an answer\", height=100)"],"metadata":{"id":"s11yNG1Lv8Pv"},"id":"s11yNG1Lv8Pv","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using a LangGraph Agent\n","To use a Prolog tool with an agent, pass it to the agent's constructor:"],"metadata":{"id":"NlYAODKRzKiA"},"id":"NlYAODKRzKiA"},{"cell_type":"code","execution_count":null,"id":"2ab5bcbc","metadata":{"id":"2ab5bcbc"},"outputs":[],"source":["from langgraph.prebuilt import create_react_agent\n","\n","tools = [planner_tool]\n","lg_agent = create_react_agent(llm, tools)"]},{"cell_type":"markdown","source":["The agent takes the query and use the Prolog tool if needed. Then the agent receives​ the tool response and generates the answer:"],"metadata":{"id":"Sd6uixbm-caw"},"id":"Sd6uixbm-caw"},{"cell_type":"code","execution_count":null,"id":"67de95df","metadata":{"id":"67de95df"},"outputs":[],"source":["verbose = False\n","tries = 1\n","inputs = {\"messages\": [(\"human\", query)]}\n","while tries <= max_tries:\n","  try:\n","    if verbose:\n","        for step in lg_agent.stream(inputs, stream_mode=\"values\"):\n","            message = step[\"messages\"][-1]\n","            message.pretty_print()\n","    else:\n","        outputs = lg_agent.invoke(inputs)\n","        results = outputs[\"messages\"][-1]\n","        results.pretty_print()\n","    break\n","  except Exception as e:\n","    tries += 1\n","if tries > max_tries:\n","    show_ai_message(\"Could not get an answer\", height=100)"]},{"cell_type":"markdown","id":"7795ebc6","metadata":{"id":"7795ebc6"},"source":["## API reference\n","\n","See https://langchain-prolog.readthedocs.io/en/latest/modules.html for detail."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}